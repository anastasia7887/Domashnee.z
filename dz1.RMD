---
title: "Кононова ПМИ 3-1"
output:

  html_document:

    df_print: paged

  html_notebook: default

  pdf_document: default

  word_document: default
---


Рассмотрим пример из задания 1: как меняется поведение ошибок на тестовой и обучающей выборках при различном числе степеней свободы, если функция зависимости отклика $Y$ от единственного признака $X$ известна. Сгенерируем $X$ и $Y$:



* $X \sim U(5, 105)$   

* $Y = f(X) + \epsilon$, где $f(X) = 15 - 0.02X + 0.0055(X-45)^2 + 6 \cdot 10^{-5} \cdot (X-54)^3$; $\epsilon \sim N(0, 1)$.  



```{r generate-data}



# ядро

my.seed <- 1486372882



# наблюдений всего

n.all <- 60

# доля обучающей выборки

train.percent <- 0.85

# стандартное отклонение случайного шума

res.sd <- 1

# границы изменения X

x.min <- 5

x.max <- 105

set.seed(my.seed)

x <- runif(x.min, x.max, n = n.all)

# случайный шум

set.seed(my.seed)

res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку

set.seed(my.seed)

inTrain <- sample(seq_along(x), size = train.percent*n.all)

y.func <- function(x) {15 + 0.2*x-0.005*((x-45)^2)+0.00006*((x-54)^3)}

x.line <- seq(x.min, x.max, length = n.all)

y.line <- y.func(x.line)

y <- y.func(x) + res

x.train <- x[inTrain]

y.train <- y[inTrain]

x.test <- x[-inTrain]

y.test <- y[-inTrain]

```



Изобразим исходные данные на графике.    



```{r plot-1, fig.height = 5, fig.width = 5}

# убираем широкие поля рисунка

par(mar = c(4, 4, 1, 1))



# наименьшие/наибольшие значения по осям

x.lim <- c(x.min, x.max)

y.lim <- c(min(y), max(y))



# наблюдения с шумом (обучающая выборка)

plot(x.train, y.train, 

     col = grey(0.2), bg = grey(0.2), pch = 21,

     xlab = 'X', ylab = 'Y', 

     xlim = x.lim, ylim = y.lim, 

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



# наблюдения тестовой выборки

points(x.test, y.test, 

       col = 'red', bg = 'red', pch = 21)



# истинная функция

lines(x.line, y.line, 

      lwd = 2, lty = 2)



# легенда

legend('topleft', legend = c('обучение', 'тест', 'f(X)'),

       pch = c(16, 16, NA), 

       col = c(grey(0.2), 'red', 'black'),  

       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

```



В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40 (количество узлов равно 2/3 наблюдений). Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках.    



```{r models}

#  Строим модель ###############################################################



# модель 2 (сплайн с df = 6)

mod <- smooth.spline(x = x.train, y = y.train, df = 6)



# модельные значения для расчёта ошибок

y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]

y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]



# считаем средний квадрат ошибки на обучающей и тестовой выборке

MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),

         sum((y.test - y.model.test)^2) / length(x.test))

names(MSE) <- c('train', 'test')

round(MSE, 2)



#  Цикл по степеням свободы ####################################################



# максимальное число степеней свободы для модели сплайна

max.df <- 40



# таблица для записи ошибок

tbl <- data.frame(df = 2:max.df)

# ошибки на обучающей выборке

tbl$MSE.train <- 0

# ошибки на тестовой выборке

tbl$MSE.test <- 0

head(tbl)



for (i in 2:max.df) {

  # модель

  mod <- smooth.spline(x = x.train, y = y.train, df = i)

  

  # модельные значения для расчёта ошибок

  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]

  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]

  

  # считаем средний квадрат ошибки на обучающей и тестовой выборке

  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),

           sum((y.test - y.model.test)^2) / length(x.test))

  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE

}



head(tbl)

```



Изобразим на графике поведение ошибок при различном количестве степеней свободы.  



```{r plot-2, fig.height = 5, fig.width = 5}

plot(x = tbl$df, y = tbl$MSE.test, 

     type = 'l', col = 'red', lwd = 2,

     xlab = 'Степени свободы сплайна', ylab = 'MSE',

     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 

              max(tbl$MSE.train, tbl$MSE.test)),

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



points(x = tbl$df, y = tbl$MSE.test,

       pch = 21, col = 'red', bg = 'red')



lines(x = tbl$df, y = tbl$MSE.train, 

      col = grey(0.3), lwd = 2)

  # неустранимая ошибка

abline(h = res.sd, 

       lty = 2, col = grey(0.4), lwd = 2)



# степени свободы у наименьшей ошибки на тестовой выборке

min.MSE.test <- min(tbl$MSE.test)

df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']



          # сообщение в консоль

            message(paste0('Наименьшая MSE на тестовой выборке равна ', 

               round(min.MSE.test, 2),  

               ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику

df.my.MSE.test <- 4

my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']

# ставим точку на графике

abline(v = df.my.MSE.test, 

       lty = 2, lwd = 2)

points(x = df.my.MSE.test, y = my.MSE.test, 

       pch = 15, col = 'blue')

mtext(df.my.MSE.test, 

      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

        # сообщение в консоль

            message(paste0('Компромисс между точностью и сложностью модели при df = ', 

               df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))



# ставим точку на графике

abline(v = df.my.MSE.test, 

       lty = 2, lwd = 2)

points(x = df.my.MSE.test, y = my.MSE.test, 

       pch = 15, col = 'blue')

mtext(df.my.MSE.test, 

      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

dev.copy(tiff ,filename="1.1.png") 

dev.off()

```



На этом графике:   



* При движении слева направо MSE на обучающей выборке (серая кривая) сокращается, потому что с ростом числа степеней свободы расчёт число узлов, по которым строится сплайн. При этом модельная кривая подгоняется по всё возрастающему количеству точек и становится всё более гибкой. В результате индивидуальные расстояния от фактических наблюдений за $Y$ до их модельных оценок сокращаются, что приводит к сокращению MSE.   

* При движении слева направо MSE на тестовой выборке (красная кривая) сначала резко сокращается, затем растёт. Нам известна истинная форма связи $Y$ с $X$, она описывается кубической функцией. Число степеней свободы такой модели равно числу оцениваемых параметров, т.е. 4 (коэффициенты перед $X$, $X^2$, $X^3$ и константа). Поэтому резкое падение ошибки на тестовой выборке при небольшом числе степеней свободы связано с тем, что модель приближается по гибкости к истинной функции связи. Затем MSE на тестовой выборке довольно долго остаётся стабильной, а затем начинает расти. Этот рост объясняется эффектом переобучения модели: она всё лучше описывает обучающую выборку, и при этом постепенно становится неприменимой ни к одному другому набору наблюдений.   



Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы `r df.min.MSE.test` и равно `r round(min.MSE.test, 2)`. Визуально по графику мы можем установить, что первое значение $MSE_{ТЕСТ}$, близкое к стабильно низким, соответствует df = `r df.my.MSE.test`. Ошибка здесь равна `r round(my.MSE.test)`, что ненамного отличается от минимума. Именно df = `r df.my.MSE.test` было выбрано в качестве компромисса между точностью (минимальной MSE на тестовой выборке) и простотой модели (чем меньше степеней свободы, тем модель проще).    



График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.   



```{r plot-3, fig.height = 5, fig.width = 5}

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)



# для гладких графиков модели

x.model.plot <- seq(x.min, x.max, length = 250)

y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]



# убираем широкие поля рисунка

par(mar = c(4, 4, 1, 1))



# наименьшие/наибольшие значения по осям

x.lim <- c(x.min, x.max)

y.lim <- c(min(y), max(y))



# наблюдения с шумом (обучающая выборка)

plot(x.train, y.train, 

     col = grey(0.2), bg = grey(0.2), pch = 21,

     xlab = 'X', ylab = 'Y', 

     xlim = x.lim, ylim = y.lim, 

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



# наблюдения тестовой выборки

points(x.test, y.test, 

       col = 'red', bg = 'red', pch = 21)



# истинная функция

lines(x.line, y.line, 

      lwd = 2, lty = 2)



# модель

lines(x.model.plot, y.model.plot, 

      lwd = 2, col = 'blue')



# легенда

legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),

       pch = c(16, 16, NA, NA), 

       col = c(grey(0.2), 'red', 'black', 'blue'),  

       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)



dev.copy(tiff ,filename="1.2.png") 

dev.off()

```



Изменим данные для второй части задания:

```{r}

# ядро

my.seed <- 1486372882



# наблюдений всего

n.all <- 60

# доля обучающей выборки

train.percent <- 0.89

# стандартное отклонение случайного шума

res.sd <- 1

# границы изменения X

x.min <- 5

x.max <- 105

set.seed(my.seed)

x <- runif(x.min, x.max, n = n.all)

# случайный шум

set.seed(my.seed)

res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку

set.seed(my.seed)

inTrain <- sample(seq_along(x), size = train.percent*n.all)

y.func <- function(x) {15 + 0.2*x-0.005*((x-45)^2)+0.00006*((x-54)^3)}

x.line <- seq(x.min, x.max, length = n.all)

y.line <- y.func(x.line)

y <- y.func(x) + res

x.train <- x[inTrain]

y.train <- y[inTrain]

x.test <- x[-inTrain]

y.test <- y[-inTrain]



# убираем широкие поля рисунка

par(mar = c(4, 4, 1, 1))



# наименьшие/наибольшие значения по осям

x.lim <- c(x.min, x.max)

y.lim <- c(min(y), max(y))



# наблюдения с шумом (обучающая выборка)

plot(x.train, y.train, 

     col = grey(0.2), bg = grey(0.2), pch = 21,

     xlab = 'X', ylab = 'Y', 

     xlim = x.lim, ylim = y.lim, 

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



# наблюдения тестовой выборки

points(x.test, y.test, 

       col = 'red', bg = 'red', pch = 21)



# истинная функция

lines(x.line, y.line, 

      lwd = 2, lty = 2)



# легенда

legend('topleft', legend = c('обучение', 'тест', 'f(X)'),

       pch = c(16, 16, NA), 

       col = c(grey(0.2), 'red', 'black'),  

       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)



#  Строим модель ###############################################################



# модель 2 (сплайн с df = 6)

mod <- smooth.spline(x = x.train, y = y.train, df = 6)



# модельные значения для расчёта ошибок

y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]

y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]



# считаем средний квадрат ошибки на обучающей и тестовой выборке

MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),

         sum((y.test - y.model.test)^2) / length(x.test))

names(MSE) <- c('train', 'test')

round(MSE, 2)

#  Цикл по степеням свободы ####################################################



# максимальное число степеней свободы для модели сплайна

max.df <- 40



# таблица для записи ошибок

tbl <- data.frame(df = 2:max.df)

# ошибки на обучающей выборке

tbl$MSE.train <- 0

# ошибки на тестовой выборке

tbl$MSE.test <- 0

head(tbl)



for (i in 2:max.df) {

  # модель

  mod <- smooth.spline(x = x.train, y = y.train, df = i)

  

  # модельные значения для расчёта ошибок

  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]

  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]

  

  # считаем средний квадрат ошибки на обучающей и тестовой выборке

  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),

           sum((y.test - y.model.test)^2) / length(x.test))

  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE

}



head(tbl)

#  График 2: Зависимость MSE от гибкости модели ################################



plot(x = tbl$df, y = tbl$MSE.test, 

     type = 'l', col = 'red', lwd = 2,

     xlab = 'Степени свободы сплайна', ylab = 'MSE',

     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 

              max(tbl$MSE.train, tbl$MSE.test)),

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



points(x = tbl$df, y = tbl$MSE.test,

       pch = 21, col = 'red', bg = 'red')



lines(x = tbl$df, y = tbl$MSE.train, 

      col = grey(0.3), lwd = 2)

# неустранимая ошибка

abline(h = res.sd, 

       lty = 2, col = grey(0.4), lwd = 2)



# степени свободы у наименьшей ошибки на тестовой выборке

min.MSE.test <- min(tbl$MSE.test)

df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']



# сообщение в консоль

message(paste0('Наименьшая MSE на тестовой выборке равна ', 

               round(min.MSE.test, 2),  

               ' и достигается при df = ', df.min.MSE.test, '.'))



# компромисс между точностью и простотой модели по графику

df.my.MSE.test <- 4

my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']

# ставим точку на графике

abline(v = df.my.MSE.test, 

       lty = 2, lwd = 2)

points(x = df.my.MSE.test, y = my.MSE.test, 

       pch = 15, col = 'blue')

mtext(df.my.MSE.test, 

      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

# сообщение в консоль

message(paste0('Компромисс между точностью и сложностью модели при df = ', 

               df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))



# ставим точку на графике

abline(v = df.my.MSE.test, 

       lty = 2, lwd = 2)

points(x = df.my.MSE.test, y = my.MSE.test, 

       pch = 15, col = 'blue')

mtext(df.my.MSE.test, 

      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)

dev.copy(tiff ,filename="2.1.png") 

dev.off()

```

ОТметим явное уменьшение МSE -среднеквадратическую ошибку модели. Так же отметим большую пологость графика зависимости MSE от степени свободы слайна, что говорит о медленном возрастании среднеквадратической ошибки с увеличением принятых степеней свободы.

```{r}

# для гладких графиков модели

x.model.plot <- seq(x.min, x.max, length = 250)

y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]



# убираем широкие поля рисунка

par(mar = c(4, 4, 1, 1))



# наименьшие/наибольшие значения по осям

x.lim <- c(x.min, x.max)

y.lim <- c(min(y), max(y))



# наблюдения с шумом (обучающая выборка)

plot(x.train, y.train, 

     col = grey(0.2), bg = grey(0.2), pch = 21,

     xlab = 'X', ylab = 'Y', 

     xlim = x.lim, ylim = y.lim, 

     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)



# наблюдения тестовой выборки

points(x.test, y.test, 

       col = 'red', bg = 'red', pch = 21)



# истинная функция

lines(x.line, y.line, 

      lwd = 2, lty = 2)



# модель

lines(x.model.plot, y.model.plot, 

      lwd = 2, col = 'blue')



# легенда

legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),

       pch = c(16, 16, NA, NA), 

       col = c(grey(0.2), 'red', 'black', 'blue'),  

       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

dev.copy(tiff ,filename="2.2.png") 

dev.off()

```

