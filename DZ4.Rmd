---

title: "R Notebook"

output:

  html_document:

    df_print: paged

  html_notebook: default

  pdf_document: default

  word_document: default

---




*Модели*: множественная линейная регрессия, kNN.   



Цель: исследовать набор данных `Boston` с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке.    



```{r Данные и пакеты, warning = F, message = F}

# загрузка пакетов

library('GGally')       # графики совместного разброса переменных

library('lmtest')       # тесты остатков регрессионных моделей

library('FNN')          # алгоритм kNN

library('MASS')

# константы

my.seed <- 12345

train.percent <- 0.85



# загрузка данных

data(Boston)  


# преобразуем категориальную переменную в фактор

Boston$chas <- as.factor(Boston$chas)



# обучающая выборка

set.seed(my.seed)

bost <- Boston[, -c(2,5,6,7,8,9,11,13,14)]

inTrain <- sample(seq_along(bost$crim), 
                  
                  nrow(bost) * train.percent)

df.train <- bost[inTrain, c(colnames(bost)[-1], colnames(bost)[1])]

df.test <- bost[-inTrain, -1]



```



## Описание переменных  



Набор данных `Boston` содержит переменные:  



- `crime` – уровень преступности на душу населения города;  

- `chas` – Фиктивная переменная реки Чарльз (**1**, Если тракт ограничивает реку; **0** в противном случае);

- `tax` – полная стоимость имущества-ставка налога за \$10,000;

- `indus` - доля не-розничных акров бизнеса в городе;

- `black` – 1000 (Bk - 0.63)^2, где Bk-доля чернокожих по городам.



Размерность обучающей выборки: $n = `r dim(df.train)[1]`$ строк, $p = `r dim(df.train)[2] - 1`$ объясняющих переменных. Зависимая переменная -- `crim`.  



```{r Описание данных-01, message = F, warning = F}

# описательные статистики по переменным

summary(df.train)

```


## Модели  



```{r , warning = F, error = F}


model.1 <- lm(crim ~ . + chas:tax + chas:black + chas:indus,
              
              data = df.train)

summary(model.1)

```

Взаимодействие `chas:tax` исключаем оно не значимо

```{r , warning = F, error = F}

summary(model.1)

model.2 <- lm(crim ~ . + chas:black + chas:indus,
              
              data = df.train)

summary(model.2)

```



Взаимодействие `chas:black` исключаем



```{r , warning = F, error = F}



model.3 <- lm(crim ~ . + chas:indus,
              
              data = df.train)

summary(model.3)


```



Коэффициент при `indus` имеет наибольшее р-значение, его исключим



```{r , warning = F, error = F}



model.4 <- lm(crim ~ chas + tax + black + chas:indus,
              
              data = df.train)

summary(model.4)



```



Коэффициент при `chas1` имеет наибольшее р-значение, его исключим



```{r , warning = F, error = F}



model.5 <- lm(crim ~ tax + black + chas:indus,
              
              data = df.train)

summary(model.5)



```



Взаимодействие `chas:indus` исключаем оно не значимо


```{r , warning = F, error = F}

model.6 <- lm(crim ~ tax + black,
              
              data = df.train)

summary(model.6)



```






Очевидно, стоит остановиться на модели без взаимодействий. Проверим её остатки. 



# Проверка остатков  



```{r , warning = F, error = F}

# тест Бройша-Пагана

bptest(model.6)



# статистика Дарбина-Уотсона

dwtest(model.6)



# графики остатков

par(mar = c(4.5, 4.5, 2, 1))

par(mfrow = c(1, 3))

plot(model.6, 1)

plot(model.6, 4)

plot(model.6, 5)

par(mfrow = c(1, 1))



```



Судя по графику слева, остатки не случайны, и их дисперсия непостоянна. Графики остатков заставляют усомниться в том, что остатки удовлетворяют условиям Гаусса-Маркова.      



# Сравнение с kNN



```{r }

# фактические значения y на тестовой выборке

y.fact <- bost[-inTrain, 1]

y.model.lm <- predict(model.6, df.test)

MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)



# kNN требует на вход только числовые переменные

df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))

df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))



for (i in 2:50){

    model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'crim')], 

                     y = df.train.num[, 'crim'], 

                     test = df.test.num, k = i)

    y.model.knn <- model.knn$pred

    if (i == 2){

        MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)

    } else {

        MSE.knn <- c(MSE.knn, 

                     sum((y.model.knn - y.fact)^2) / length(y.model.knn))

    }

}



# график

par(mar = c(4.5, 4.5, 1, 1))

# ошибки kNN

plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',  ylim = c(165, 200),

     xlab = 'значение k', ylab = 'MSE на тестовой выборке')

# ошибка регрессии

lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)

legend('bottomright', lty = c(1, 2), pch = c(1, NA), 

       col = c('darkgreen', grey(0.2)), 

       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 

       lwd = rep(2, 2))

```



```{r, include = F}

frml.to.text.01 <- paste0('$\\frac {\\sqrt{MSE_{TEST}}}{\\bar{y}_{TEST}} = ',

                          round(sqrt(MSE.lm) / mean(y.fact) * 100, 1),

                          '\\%$')

```





Как можно видеть по графику, ошибка регрессии на тестовой выборке больше, чем ошибка метода k ближайших соседей с k от 2 до 20. Далее с увеличением количества соседей точность kNN падает. Ошибка регрессионной модели на тестовой выборке очень велика и составляет `r frml.to.text.01` от среднего значения зависимой переменной. Для модели регрессии это может означать отсутствие важного объясняющего фактора. У лучшей модели kNN также низкая точность: она ошибается на `r paste0(round(sqrt(min(MSE.knn)) / mean(y.fact) * 100, 1), '%')` от среднего значения объясняющей переменной.     

