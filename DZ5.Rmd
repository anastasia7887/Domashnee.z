---

title: "R Notebook"

output:

  html_document:

    df_print: paged

  html_notebook: default

  pdf_document: default

  word_document: default

---
*Модели*: линейная регрессия, kNN.  

 Первая модель всключает переменные:
 - `crime` – уровень преступности на душу населения города, зависимая переменная;  

- `chas` – Фиктивная переменная реки Чарльз (**1**, Если тракт ограничивает реку; **0** в противном случае);

- `tax` – полная стоимость имущества-ставка налога за \$10,000;

- `indus` - доля не-розничных акров бизнеса в городе;

- `black` – 1000 (Bk - 0.63)^2, где Bk-доля чернокожих по городам.

Вторая модель содержит те же переменные, кроме фактора `chas`.

*Данные*: `Boston {MASS}`   

```{r Данные и пакеты, warning = F, message = F}
library('GGally')            # матричные графики
library('boot') 
library('MASS')
library('FNN')          # алгоритм kNN
data(Boston)     

Boston$chas <- as.factor(Boston$chas)

bost <- Boston[, -c(2,5,6,7,8,9,11,13,14)]

# доля обучающей выборки
train.percent <- 0.5
my.seed <- 12345

n <- nrow(bost)

```

## Метод проверочной выборки 



Он состоит в том, что мы отбираем одну тестовую выборку и будем считать на ней ошибку модели.    

```{r, warning = F, error = F}
# доля обучающей выборки
train.percent <- 0.5

# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
attach(bost)
inTrain <- sample(n, n*train.percent)

lm.1 <- lm(crim ~ ., subset = inTrain, data = bost)
mean((crim[-inTrain] - predict(lm.1, bost[-inTrain, ]))^2)

detach(bost)

attach(bost)

lm.2 <- lm(crim ~ tax + black + indus, subset = inTrain, data = bost)
mean((crim[-inTrain] - predict(lm.2, bost[-inTrain, ]))^2)

detach(bost)
```

Этот метод советует нам первую модель.


### Перекрёстная проверка по отдельным наблюдениям (LOOCV)



Это самый затратный в вычислительном плане метод, но и самый надёжный в плане оценки ошибки вне выборки. Попробуем применить его к линейной модели.    



```{r, warning = F, error = F}

# подгонка линейной модели на обучающей выборке
fit.glm.1 <- glm(crim ~ ., data = bost)
cv.err <- cv.glm(bost, fit.glm.1)

# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err$delta

fit.glm.2 <- glm(crim ~ tax + black + indus, data = bost)
cv.err <- cv.glm(bost, fit.glm.2)

# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err$delta

```

Данный метод так же советует нам первую модель.

### k-кратная перекрёстная проверка



K-кратная кросс-валидация -- компромисс между методом проверочной выборки и LOOCV. Оценка ошибки вне выборки ближе к правде, по сравнению с проверочной выборкой, а объём вычислений меньше, чем при LOOCV. Проведём 10-кратную кросс-валидацию моделей разных степеней.     

```{r, warning = F, error = F}
# k-кратная перекрёстная проверка ==============================================
#Модель со всеми предикторами
#k=10

fit.glm.1 <- glm(crim ~ ., data = bost)
cv.err.k.fold <- cv.glm(bost, fit.glm.1, K = 10)$delta[1]

cv.err.k.fold

#Модель без факторов

#k=10

fit.glm.2 <- glm(crim ~ tax + black + indus, data = bost)
cv.err.k.fold <- cv.glm(bost, fit.glm.1, K = 10)$delta[1]

cv.err.k.fold

```
Первый результат - десятикратная перекрестная проверка для модели 1 (со всеми предикторами)
Второй -десятикратная перекрестная проверка для 2 модели(без факторов).
В данном случае рекомендуется 1 модель.

```{r, warning = F, error = F}


#k=5

fit.glm.1 <- glm(crim ~ ., data = bost)
cv.err.k.fold <- cv.glm(bost, fit.glm.1, K = 5)$delta[1]

cv.err.k.fold

#k=5

fit.glm.2 <- glm(crim ~ tax + black + indus, data = bost)
cv.err.k.fold <- cv.glm(bost, fit.glm.1, K = 5)$delta[1]

cv.err.k.fold
```
Первый результат - пятикратная перекрестная проверка для модели 1 (со всеми предикторами)
Второй -пятикратная перекрестная проверка для 2 модели(без факторов).

Этот способ же рекомендует вторую модель.


## Бутстреп   

### Точность оценки параметра регрессии   



При построении модели регрессии проблемы в остатках приводят к неверной оценке ошибок параметров. Обойти эту проблему можно, применив для расчёта этих ошибок бутстреп.   

Возьмем модель 1 включающую в себя все переменные.
```{r, warning = F, error = F}
# функция для расчёта коэффициентов ПЛР по выборке из данных
boot.fn <- function(data, index){
  coef(lm(crim ~ ., data = data, subset = index))
}
boot.fn(bost, 1:n)    



# пример применения функции к бутстреп-выборке
set.seed(my.seed)
boot.fn(bost, sample(n, n, replace = T))


# применяем функцию boot для вычисления стандартных ошибок параметров
#  (1000 выборок с повторами)
boot(bost, boot.fn, 1000)

# сравним с МНК
attach(bost)
summary(lm(crim ~ ., data = bost))$coef
detach(bost)

# оценки не отличаются 

```
Оценки для МНК и бутстрепа в данном случае не отличаются